---
title: "Seminar 6"
author: "Elena Rybina 'MKH 161'"
date: "2/21/2018"
output:
  pdf_document: default
  html_notebook: default
---

```{r libraries, echo = FALSE, message=F, warning=FALSE}
#install.packages("NetData")
# install.packages("nFactors")
# install.packages("lattice")
library('NetData')
#library(network)
#library(sna)
library(RColorBrewer) 
library(NetData)
library(nFactors)
#detach(package:sna) 

```



```{r}
data(studentnets.M182, package = "NetData")
head(m182_full_data_frame)


m182_full_nonzero_edges <- subset(m182_full_data_frame, (friend_tie > 0 | social_tie > 0 | task_tie > 0))
head( m182_full_nonzero_edges) 
```


```{r, echo = FALSE, message=F, warning=FALSE}
library(igraph)

```


```{r}
m182_full <- graph.data.frame(m182_full_nonzero_edges) 
str(m182_full)
m182_full
summary(m182_full) 

```

```{r, fig.align='center'}
m182_friend <- delete.edges(m182_full, E(m182_full)[E(m182_full)$friend_tie==0]) 
m182_friend
plot(m182_friend)
m182_social <- delete.edges(m182_full, E(m182_full)[E(m182_full)$social_tie==0]) 
m182_social
plot(m182_social)
m182_task <- delete.edges(m182_full, E(m182_full)[E(m182_full)$task_tie==0])
m182_task
plot(m182_task)



```


*******   

_**Assignment question.**_ *Why do we remove the zero edges from networks? We havenâ€™t done it previously, why are we doing it now?*   
Probably it is because of our interest in particular types of tie and their interaction: we are going to find patterns in ties of more than one type, and data wherein all of these types of ties are absence will be meaningless for us.   

*******   


```{r}
# This is if we want to use the edge value
task_adjacency<-get.adjacency(m182_task, attr='task_tie') # This is if we only want the tie (so it's 0 or 1)
str(task_adjacency)
head(task_adjacency)
binary_task_adjacency<-get.adjacency(m182_task)
head(binary_task_adjacency)
```
##Clustering directed data

```{r}
task_adjacency<-as.matrix(task_adjacency) #generate the matrix out of a graph 
# Create a nx2n matrix of directed connections 
task_matrix<-rbind(task_adjacency,t(task_adjacency))


# Same for matrix of social connections:
social_adjacency<-get.adjacency(m182_social, attr='social_tie') 
binary_social_adjacency<-get.adjacency(m182_social) #this is for later
social_adjacency<-as.matrix(social_adjacency) 
social_matrix<-rbind(social_adjacency,t(social_adjacency))

# Because we want to analyze social and task connections together, bind matrices:
task_social_matrix <-rbind(task_matrix,social_matrix) 
dim(task_social_matrix)
head(task_social_matrix)
```



```{r}
task_social_cors<-cor(task_social_matrix) # Correlate matrices
head(task_social_cors )

 dissimilarity<-1-task_social_cors #subtract matrix values from 1 
 task_social_dist<-as.dist(dissimilarity) #create a distance matrix
 
  #You can check the matrix if you wish:
##task_social_dist
 
 task_social_dist<-dist(t(task_social_matrix))
 
 library(NetCluster) # add the library to complete the clustering
 
  task_social_hclust <- hclust(task_social_dist)
  plot(task_social_hclust)
  
  

```


```{r}
cutree(task_social_hclust,k=2)
cutree(task_social_hclust,k=6)
```

```{r}
clustered_observed_cors = vector() # set it as a vector 
num_vertices = length(V(m182_task)) # get the length of the vector

clustered_observed_cors <-clustConfigurations(num_vertices,task_social_hclust,task_social_cors)
```

```{r}
clustered_observed_cors$correlations
#number of explained variance
```
4 clusters:
```{r}
num_clusters = 4 
clusters <- cutree(task_social_hclust, k = num_clusters) 
clusters
cluster_cor_mat <- clusterCorr(task_social_cors, clusters)
gcor(cluster_cor_mat, task_social_cors)
```

6 clusters:
```{r}
num_clusters = 6 
clusters <- cutree(task_social_hclust, k = num_clusters) 
clusters
cluster_cor_mat <- clusterCorr(task_social_cors, clusters)
gcor(cluster_cor_mat, task_social_cors)
```
*******   

_**Assignment questions.**_ *What rationale do you have for selecting the number of clusters / positions with the method above? Please rely on your knowledge of cluster analysis to answer this question.*   

The choice of a number of clusters is based on the ratio between percentage of explained variance and number of clusters, so that adding of new cluster will not strongly increase number of explained variance. Through the plot "number of clusters / percentage of variance" we can use "elbow method" and try to find the point where is line does the most acute angle.    

*******  


```{r}
apriori = c(1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3) 
deductive_cluster_cor_mat <- generate_cluster_cor_mat(task_social_cors, apriori) 

#calculate amount of expelaned var by yourself for your model
gcor(deductive_cluster_cor_mat, task_social_cors)
```

##Back to blockmodeling   


```{r}
# Blockmodel on valued task data
task_valued_blockmodel <- blockmodel(task_adjacency, clusters)
# Blockmodel on binary task data 
binary_task_adjacency<-as.matrix(binary_task_adjacency) # turn graph to matrix first 
task_binary_blockmodel <- blockmodel(binary_task_adjacency, clusters)
# Blockmodel on valued social data
social_valued_blockmodel <- blockmodel(social_adjacency, clusters)
#Blockmodel on binary social data
binary_social_adjacency<-as.matrix(binary_social_adjacency) 
social_binary_blockmodel <- blockmodel(binary_social_adjacency, clusters)
# Now, look at the basic statistics:
task_mean <- mean(task_adjacency) 
task_mean

task_density <- graph.density(m182_task) 
task_density

social_mean <- mean(social_adjacency) 
social_mean

social_density <- graph.density(m182_social) 
social_density
```

*******   

_**Assignment task and questions.**_ *1. Plot the resulting blockmodels in any way you wish and examine them visually. What is the story you get from viewing these clusters, and their within and between cluster densities on task and social interaction? What can you say about your network from this? 2. We have learned several ways to blockmodel. Which method do you find the most intuitively appealing? Why? 3. What did you learn from blockmodels about your data that you could not generate from previously learned techniques?*    

  

```{r, echo = FALSE, message=F, warning=FALSE}
detach(package:igraph) 
library(sna)
```

```{r}
# Blockmodel task data
plot.blockmodel(task_valued_blockmodel)
plot.blockmodel(task_binary_blockmodel)


# Blockmodel social data
plot.blockmodel(social_valued_blockmodel)
plot.blockmodel(social_binary_blockmodel)
```
    
1. We can see from the densities and blockmodels that social network more tight itself and its clusters more tight also. It means that students has more social connections than connections based on shared tasks.    
2. I can say that for is more intuitively appealing blockmodelling based on cluster analysis on euclidian distance. I suppose, one of the reasons is that this method more familiar to me, but probably it is more understandable and visually intuituve itself.   
3. When we are going to do blockmodel based in different types of tie, we have to prepare data and take the data wherein at least one of type of connections is present. Otherwise, our data will be meaningless. For such type of blcokmodeling we have to combine adjacency matrixes of all types of ties in whch we are interested.

*******   


##Clustering based on PCA

```{r}
ev <- eigen(cor(task_social_matrix)) # get eigenvalues 
ap <- parallel(subject=nrow(task_social_matrix), 
               var=ncol(task_social_matrix),
               rep=100,cent=.05)
nS <- nScree(ev$values, ap$eigen$qevpea)
plotnScree(nS)
```

```{r}
pca_task_social = principal(task_social_matrix, nfactors=6, rotate="varimax")
# Look at the results. Most of it should look familiar:
pca_task_social

pca_task_social$values
```
```{r}
# Take the distance based on Euclidian Distance
task_factor_dist = dist(pca_task_social$loadings)
# And cluster
task_factor_hclust <- hclust(task_factor_dist) 
plot(task_factor_hclust)
```

```{r}

par(mfrow = c(1,2))
plot(task_social_hclust, main = "Correlation") 
plot(task_factor_hclust, main = "PCA")
```
******    


_*Assignment questions.**_ *1. How do the results across blockmodel techniques differ? Why might you use one over the other? 2. Why might you want to run more than one in your analyses?*    

1. Cluster analysis is based on not firm assumptions. It is alsways difficult to split data into clear clusters, and as far as I understand, for SNA this problem is even stronger.  Different methods of splitting data into separate blocks use different algorithms and work on different mathematical rationales. And these metods will return different result. If all of methods return completely different clusters -- probably, our data couldn't be clusterized. If there is a tend through the methods and at least some clusters are the some -- it is good result.
2. So, the best solution is to try different methods and to look for a common tend.   

******    


## 3-D plotting

```{r}
library(RColorBrewer)
library(ergm)
library(sna)
#library(rgl)
load('madmen.Rdata')
mad.net <- as.network(mad.matrix, directed=FALSE)
mad.net
col20 <- c(brewer.pal(11,'RdYlBu'),brewer.pal(9,'RdYlBu'))
# gplot3d(mad.net, vertex.col=col20, vertex.radius=1.5, edge.lwd=.2)
```
Something is wrong with rgl package :(

*"Error: package or namespace load failed for 'rgl': .onLoad failed in loadNamespace() for 'rgl', details: call: NULL error: Loading rgl's DLL failed. On MacOS, rgl depends on XQuartz"*
